{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mines Nancy - Fall 2024 - Numerical Optimization \n",
    "----\n",
    "## TP2: descent methods for unconstrained optimization problems\n",
    "### Part 1: gradient descent methods\n",
    "\n",
    "In this notebook, we will explore first-order descent methods -- namely **gradient descent** to solve (unconstrained) optimization problems. The goals are:  \n",
    "- to write your own gradient descent algorithm;\n",
    "- investigate different step-size strategies (fixed size, optimal, and backtracking)\n",
    "- compute convergence metrics to monitor the progress of the algorithm -- and deciding when to stop\n",
    "\n",
    "Just like in TP1, practical implementation of the algorithms relies on classical Python libraries:\n",
    "1. **NumPy**: For efficient numerical computations, matrix operations, and solving least-squares problems using built-in linear algebra functions.\n",
    "2. **Matplotlib**: For visualizing the data and displaying / interpreting results.\n",
    "3. **Scipy**: a NumPy-based python library for scientific computations. In particular, we we'll use some functions from ``scipy.optimize``. \n",
    "\n",
    "### Running the Notebook\n",
    "This notebook can be executed in the following environments:\n",
    "- **Google Colab**: A convenient, cloud-based option that requires no setup. Simply open the notebook in Colab, and you're ready to run the code.\n",
    "- **Locally**: You can run the notebook on your local machine using environments like **JupyterLab** or **Jupyter Notebook**. You can also run it directly in **Visual Studio Code** if you have the Jupyter extension. In all cases, ensure you have Python installed along with the required libraries, `NumPy` and `Matplotlib` and ``scipy``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First example: a quadratic function\n",
    "\n",
    "Consider the following optimization problem\n",
    "$$ \\min_{\\mathbf{x}\\in \\mathbb{R}^2} f(\\mathbf{x})$$ \n",
    "where \n",
    "$$f(\\mathbf{x}) = \\frac{1}{2} \\mathbf{x}^\\top \\mathbf{Q} \\mathbf{x} - \\mathbf{p}^\\top \\mathbf{x} \\text{ with } \\begin{cases}Â \\mathbf{Q} = \\begin{bmatrix} 1 & 0 \\\\ 0 & \\eta\\end{bmatrix}, \\: \\eta > 0\\\\\n",
    "\\mathbf{p} = \\begin{bmatrix} 1\\\\1\\end{bmatrix}\\end{cases}$$\n",
    "\n",
    "**Preliminary questions**\n",
    "1. Recall the expression of $\\nabla f(\\mathbf{x})$. \n",
    "2. What is a minimizer of this optimization problem? Is it unique?\n",
    "3. Recall the principle of gradient descent using pseudo-code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> write here your answers using markdown. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programming questions**\n",
    "1. Create a function that returns the value of the objective function for a given vector $\\mathbf{x} = [x_1, x_2]$, for $\\eta = 5$. \n",
    "2. Display the objective function on the grid $[-1, 1] \\times [-1, 1]$, with, say 100 points on each axis. (check the functions ``np.meshgrid``, ``plt.pcolormesh``, ``plt.contour``, ``plt.contourf``). Comment the different graphical representations. \n",
    "3. Gradient descent (*constant step-size*)\n",
    "   1. write an algorithm that performs constant step-size gradient descent for a fixed number of iterations from a initial point $\\mathbf{x}_0$. \n",
    "   2. display such iterations on top of the cost function. \n",
    "   3. propose several metrics to assess the progress (or convergence) of the algorithm as the iterations goes. Display them as a function of iterations (using ``plt.semilogy``)\n",
    "   4. modify your algorithm to stop whenever one of these metrics goes below a prescribed threshold (fixed in advance). \n",
    "   5. Using numerical experiments, comment on how to choose a constant step size.\n",
    "4. Gradient descent (*optimal step-size*)\n",
    "   1. recall the principle of the optimal stepsize strategy. \n",
    "   2. adapt the previous algorithm to compute the optimal stepsize at each iteration. \n",
    "   3. display the evolution of performance metrics along iterations. \n",
    "\n",
    "5. Compare both strategies in terms of convergence speed and comment. How does the parameter $\\eta$ influences algorithmic properties? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second example: a non-quadratic function\n",
    "Example from Boyd and Vanderberghe. \n",
    "\n",
    "Consider the optimization problem\n",
    "$$ \\min_{x_1, x_2} f(x_1, x_2)$$ \n",
    "where\n",
    "$$f(x_1, x_2)= e^{x_1 + 3x_2 - 0.1} + e^{x_1 - 3x_2 - 0.1} + e^{-x_1 - 0.1}$$\n",
    "\n",
    "In this example, we'll implement the backtracking method seen in class. \n",
    "\n",
    "**Preliminary questions**\n",
    "1. Compute the expression of $\\nabla f(\\mathbf{x})$. \n",
    "2. Is the problem convex? Comment the existence and uniqueness of solutions to this optimization problem. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> write here your answers using markdown. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programming questions**\n",
    "1. Create a function that returns the value of the objective function for a given vector $\\mathbf{x} = [x_1, x_2]$. \n",
    "2. Display the objective function on the grid $[-2, 0.5] \\times [-0.5, 0.5]$, with, say 100 points on each axis. \n",
    "3. Gradient descent (*constant step-size* and *optimal step size*)\n",
    "   1. Following the quadratic example, write a gradient stepsize algorithm for this problem.\n",
    "   2. Write a second algorithm with optimal step size selection at each iteration. Do not try to compute the optimal step analytically; rather, look at the function ``scipy.optimize.minimize_scalar`` to compute the step numerically. \n",
    "\n",
    "4. Gradient descent (*backtracking*)\n",
    "   1. Recall the principle of backtracking using pseudocode. \n",
    "   2. For the first gradient descent iteration, implement the backtracking procedure seen in class for arbitrary parameters $(s, \\eta)$. \n",
    "   3. How does the amount of backtracks varies as parameters $(s, \\eta)$ are changed?\n",
    "   4. Incorporate the backtracking strategy in the gradient descent algorithm. Add a counter that tracks the number of backtracks in the algorithm and monitor this information. \n",
    "5. Compare all three approaches trough selected graphical plots. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: second-order descent methods\n",
    "\n",
    "Consider again the second optimization problem, i.e., \n",
    "\n",
    "$$ \\min_{x_1, x_2} f(x_1, x_2)$$ \n",
    "where\n",
    "$$f(x_1, x_2)= e^{x_1 + 3x_2 - 0.1} + e^{x_1 - 3x_2 - 0.1} + e^{-x_1 - 0.1}$$\n",
    "\n",
    "**Preliminary questions**\n",
    "1. compute the Hessian of $f$\n",
    "2. using pseudo-code, recall the principle of Newton methods and quasi-Newton methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> write here your answers using markdown. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Programming questions**\n",
    "1. Implement Newton's method for this problem. \n",
    "2. Implement the BFGS algorithm for this problem. For the first iteration, use $B_0 = \\nabla^2 f(x^{(0)})$ to initialize the method. \n",
    "3. Compare gradient descent, Newton's algorithm and BFGS through numerical experiments. Evaluate convergence speed, numerical complexity and timings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
